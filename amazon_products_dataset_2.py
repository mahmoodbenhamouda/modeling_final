# -*- coding: utf-8 -*-
"""amazon_products_dataset_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fi9CrcXJp-Rad6DPCo7MtD2niB1Mga_H

# **DATA UNDERSTANDING**
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/ds/train.csv')
df

import pandas as pd

file_path = '/content/drive/MyDrive/ds/train.csv'  # Update the path
df = pd.read_csv(file_path)
print(df.head())  # Display the first few rows

print(df['DESCRIPTION'].head())  # First 5 rows

print(df['DESCRIPTION'].sample(100))  # Shows 5 random descriptions

print(f"Number of unique descriptions: {df['DESCRIPTION'].nunique()}")

print(df['DESCRIPTION'])

df.info()

df.describe()

df.describe(include='all')

print("Column Names:", df.columns.tolist())  # List all column names
print("Data Types:\n", df.dtypes)  # Show data types of each column

print("Missing Values:\n", df.isnull().sum())  # Count missing values per column

print(df['DESCRIPTION'].count())  # Counts non-null values in the DESCRIPTION column

available_descriptions = len(df) - 1157382
print(available_descriptions)

import pandas as pd
import ipywidgets as widgets
from IPython.display import display

# Convert non-null descriptions to a list
descriptions = df[df['DESCRIPTION'].notnull()]['DESCRIPTION'].tolist()

# Create a text area widget with a scrollbar
text_area = widgets.Textarea(
    value="\n\n".join(descriptions[:100]),  # Show first 100 descriptions
    placeholder='No descriptions available',
    description='Descriptions:',
    layout=widgets.Layout(width='100%', height='300px')  # Adjust size
)

display(text_area)

print(df['TITLE'])

# Calculer la longueur du texte pour chaque colonne
df['TITLE_LENGTH'] = df['TITLE'].str.len()
df['DESCRIPTION_LENGTH'] = df['DESCRIPTION'].str.len()
df['BULLET_POINTS_LENGTH'] = df['BULLET_POINTS'].str.len()

# Afficher les statistiques descriptives pour les longueurs
print(df[['TITLE_LENGTH', 'DESCRIPTION_LENGTH', 'BULLET_POINTS_LENGTH']].describe())

import matplotlib.pyplot as plt
import seaborn as sns

# Afficher les histogrammes
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.histplot(df['TITLE_LENGTH'], bins=50, kde=True)
plt.title('Distribution de la longueur des TITRES')

plt.subplot(1, 3, 2)
sns.histplot(df['DESCRIPTION_LENGTH'], bins=50, kde=True)
plt.title('Distribution de la longueur des DESCRIPTIONS')

plt.subplot(1, 3, 3)
sns.histplot(df['BULLET_POINTS_LENGTH'], bins=50, kde=True)
plt.title('Distribution de la longueur des BULLET POINTS')

plt.show()

import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Fonction pour nettoyer les balises HTML
def clean_html(text):
    clean_text = re.sub(r'<.*?>', '', text)  # Supprimer les balises HTML
    return clean_text

# Appliquer le nettoyage sur les descriptions et titres
df_sample['cleaned_description'] = df_sample['DESCRIPTION'].apply(lambda x: clean_html(str(x)))
df_sample['cleaned_title'] = df_sample['TITLE'].apply(lambda x: clean_html(str(x)))

# Préparer le texte nettoyé pour les titres
title_text_cleaned = " ".join(df_sample['cleaned_title'].dropna())

# Supprimer le mot "nun" du texte nettoyé pour les titres
title_text_cleaned = re.sub(r'\bnun\b', '', title_text_cleaned)

# Créer le nuage de mots pour les titres nettoyés
wordcloud_title_cleaned = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(title_text_cleaned)

# Afficher le nuage de mots pour les titres nettoyés
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_title_cleaned, interpolation='bilinear')
plt.axis("off")
plt.title("Nuage de mots des Titres")
plt.show()

import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Fonction pour nettoyer les balises HTML
def clean_html(text):
    clean_text = re.sub(r'<.*?>', '', text)  # Supprimer les balises HTML
    return clean_text

# Appliquer le nettoyage sur les descriptions et titres
df_sample['cleaned_description'] = df_sample['DESCRIPTION'].apply(lambda x: clean_html(str(x)))
df_sample['cleaned_title'] = df_sample['TITLE'].apply(lambda x: clean_html(str(x)))

# Préparer le texte nettoyé pour les descriptions
desc_text_cleaned = " ".join(df_sample['cleaned_description'].dropna())

# Supprimer le mot "nun" du texte nettoyé
desc_text_cleaned = re.sub(r'\bnun\b', '', desc_text_cleaned)

# Créer le nuage de mots pour les descriptions nettoyées
wordcloud_desc_cleaned = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(desc_text_cleaned)

# Afficher le nuage de mots pour les descriptions nettoyées
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_desc_cleaned, interpolation='bilinear')
plt.axis("off")
plt.title("Nuage de mots des Descriptions")
plt.show()

from collections import Counter

# Fonction pour obtenir les mots les plus fréquents
def get_most_common_words(text_data):
    words = " ".join(text_data).split()
    word_counts = Counter(words)
    return word_counts.most_common(10)

# Obtenir les mots les plus fréquents dans les titres
title_words_freq = get_most_common_words(df_sample['cleaned_title'].dropna())

# Créer un graphique en barres pour les mots les plus fréquents dans les titres
title_words, title_counts = zip(*title_words_freq)
plt.figure(figsize=(10, 5))
plt.barh(title_words, title_counts, color='blue')
plt.xlabel("Fréquence")
plt.title("Mots les plus fréquents dans les Titres")
plt.gca().invert_yaxis()
plt.show()

# Obtenir les mots les plus fréquents dans les descriptions
desc_words_freq = get_most_common_words(df_sample['cleaned_description'].dropna())

# Créer un graphique en barres pour les mots les plus fréquents dans les descriptions
desc_words, desc_counts = zip(*desc_words_freq)
plt.figure(figsize=(10, 5))
plt.barh(desc_words, desc_counts, color='green')
plt.xlabel("Fréquence")
plt.title("Mots les plus fréquents dans les Descriptions")
plt.gca().invert_yaxis()
plt.show()

"""# **DATA PREPARATION**"""

# Supprimer les colonnes PRODUCT_TYPE_ID et PRODUCT_LENGTH
df_cleaned = df.drop(columns=['PRODUCT_TYPE_ID', 'PRODUCT_LENGTH','TITLE_LENGTH','DESCRIPTION_LENGTH','BULLET_POINTS_LENGTH'])

# Afficher les premières lignes pour vérifier
print(df_cleaned.head())

import pandas as pd

# Assuming your DataFrame is named df
# Drop rows where the 'DESCRIPTION' column is null
df_cleaned = df_cleaned.dropna(subset=['DESCRIPTION'])



print(df_cleaned)

from huggingface_hub import login

login()  # This will ask you to enter your Hugging Face API token

# Select the first 10 rows (or any other sample size)
df_sample = df_cleaned.head(10).copy()  # Use .copy() to avoid warnings

# Load Amazon Science's product NER model (requires authentication if private)
from transformers import pipeline

# Load the publicly available model
ner_pipeline = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")

# Test the NER extraction
def extract_product_name(text):
    entities = ner_pipeline(text)
    product_entities = [ent['word'] for ent in entities if ent['entity_group'] in ['MISC', 'CONSUMER_GOOD']]
    return " ".join(product_entities).strip()

# Apply this function to your DataFrame
df_sample["actual_product_name"] = df_sample["TITLE"].apply(extract_product_name)
print(df_sample[["TITLE", "actual_product_name"]].head(10))

!pip install transformers

from transformers import pipeline

# Load the pre-trained NER pipeline
ner_pipeline = pipeline("ner", grouped_entities=True)

import torch
print(torch.cuda.is_available())  # Should return True if a GPU is available

df_cleaned.info()